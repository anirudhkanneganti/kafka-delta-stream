{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4741506",
   "metadata": {},
   "source": [
    "#### Print Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15cf7d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.4.4-bin-hadoop3-scala2.13\n",
      "C:\\kafka\\kafka_2.13-3.9.0\n",
      "C:\\hadoop-3.3.6\n",
      "C:\\Program Files\\Java\\jdk-1.8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"SPARK_HOME\"))\n",
    "print(os.environ.get(\"KAFKA_HOME\"))\n",
    "print(os.environ.get(\"HADOOP_HOME\"))\n",
    "print(os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa5bcc",
   "metadata": {},
   "source": [
    "#### Spark Session with Kafka Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7966d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.4.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"KafkaRetailConsumer\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Spark Version:\", spark.version)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44320ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"KafkaRetailConsumer\")\n",
    "    .master(\"local[*]\")\n",
    "    # .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    # .config(\"spark.sql.streaming.schemaInference\", \"true\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.4,\"\n",
    "            \"org.apache.spark:spark-token-provider-kafka-0-10_2.13:3.4.4,\"\n",
    "            \"org.apache.kafka:kafka-clients:3.5.1,\"\n",
    "            \"org.apache.commons:commons-pool2:2.11.1,\"\n",
    "            \"io.delta:delta-spark_2.13:2.4.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3abe9",
   "metadata": {},
   "source": [
    "#### Read from Kafka Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba974314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"retail_orders_us,retail_orders_in,retail_orders_others\")\n",
    "    .option(\"startingOffsets\", \"earliest\")  # \"latest\" if you only want new ones\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32ef2a",
   "metadata": {},
   "source": [
    "#### Parse The JSON Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100fe56",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StringType, IntegerType, FloatType, TimestampType\n\u001b[32m      4\u001b[39m schema = StructType() \\\n\u001b[32m      5\u001b[39m     .add(\u001b[33m\"\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m\"\u001b[39m, StringType()) \\\n\u001b[32m      6\u001b[39m     .add(\u001b[33m\"\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m\"\u001b[39m, StringType()) \\\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     .add(\u001b[33m\"\u001b[39m\u001b[33mchannel\u001b[39m\u001b[33m\"\u001b[39m, StringType()) \\\n\u001b[32m     11\u001b[39m     .add(\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m, StringType())\n\u001b[32m     13\u001b[39m df_parsed = (\n\u001b[32m     14\u001b[39m     df_raw\n\u001b[32m     15\u001b[39m     .selectExpr(\u001b[33m\"\u001b[39m\u001b[33mCAST(value AS STRING)\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpartition\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moffset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mjsonData\u001b[39m\u001b[33m\"\u001b[39m, from_json(col(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m), schema))\n\u001b[32m     17\u001b[39m     .select(\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpartition\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moffset\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m         \u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjsonData.*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquantity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquantity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, FloatType, TimestampType\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"order_id\", StringType()) \\\n",
    "    .add(\"product_id\", StringType()) \\\n",
    "    .add(\"quantity\", StringType()) \\\n",
    "    .add(\"price\", StringType()) \\\n",
    "    .add(\"country\", StringType()) \\\n",
    "    .add(\"channel\", StringType()) \\\n",
    "    .add(\"timestamp\", StringType())\n",
    "\n",
    "df_parsed = (\n",
    "    df_raw\n",
    "    .selectExpr(\"CAST(value AS STRING)\", \"topic\", \"partition\", \"offset\")\n",
    "    .withColumn(\"jsonData\", from_json(col(\"value\"), schema))\n",
    "    .select(\n",
    "        \"topic\", \"partition\", \"offset\",\n",
    "        col(\"jsonData.*\"))\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"float\"))\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf92f0",
   "metadata": {},
   "source": [
    "#### Write Stream to Delta - Partitioned by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c5829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+------+--------+----------+--------+-----+-------+-------+--------------------+\n",
      "|topic           |partition|offset|order_id|product_id|quantity|price|country|channel|timestamp           |\n",
      "+----------------+---------+------+--------+----------+--------+-----+-------+-------+--------------------+\n",
      "|retail_orders_us|0        |0     |ORD001  |P106      |NULL    |NULL |US     |mobile |2025-04-07T10:00:00Z|\n",
      "|retail_orders_us|0        |1     |ORD004  |P108      |NULL    |NULL |US     |online |2025-04-07T10:00:45Z|\n",
      "|retail_orders_us|0        |2     |ORD007  |P125      |NULL    |NULL |US     |mobile |2025-04-07T10:01:30Z|\n",
      "|retail_orders_us|0        |3     |ORD009  |P141      |NULL    |NULL |US     |store  |2025-04-07T10:02:00Z|\n",
      "|retail_orders_us|0        |4     |ORD013  |P111      |NULL    |NULL |US     |online |2025-04-07T10:03:00Z|\n",
      "|retail_orders_us|0        |5     |ORD014  |P149      |NULL    |NULL |US     |mobile |2025-04-07T10:03:15Z|\n",
      "|retail_orders_us|0        |6     |ORD015  |P107      |NULL    |NULL |US     |online |2025-04-07T10:03:30Z|\n",
      "|retail_orders_us|0        |7     |ORD021  |P136      |NULL    |NULL |US     |mobile |2025-04-07T10:05:00Z|\n",
      "|retail_orders_us|0        |8     |ORD028  |P125      |NULL    |NULL |US     |online |2025-04-07T10:06:45Z|\n",
      "|retail_orders_us|0        |9     |ORD034  |P121      |NULL    |NULL |US     |store  |2025-04-07T10:08:15Z|\n",
      "|retail_orders_us|0        |10    |ORD035  |P108      |NULL    |NULL |US     |mobile |2025-04-07T10:08:30Z|\n",
      "|retail_orders_us|0        |11    |ORD043  |P104      |NULL    |NULL |US     |mobile |2025-04-07T10:10:30Z|\n",
      "|retail_orders_us|0        |12    |ORD053  |P103      |NULL    |NULL |US     |store  |2025-04-07T10:13:00Z|\n",
      "|retail_orders_us|0        |13    |ORD054  |P129      |NULL    |NULL |US     |store  |2025-04-07T10:13:15Z|\n",
      "|retail_orders_us|0        |14    |ORD059  |P127      |NULL    |NULL |US     |store  |2025-04-07T10:14:30Z|\n",
      "|retail_orders_us|0        |15    |ORD063  |P104      |NULL    |NULL |US     |online |2025-04-07T10:15:30Z|\n",
      "|retail_orders_us|0        |16    |ORD085  |P117      |NULL    |NULL |US     |online |2025-04-07T10:21:00Z|\n",
      "|retail_orders_us|0        |17    |ORD091  |P135      |NULL    |NULL |US     |online |2025-04-07T10:22:30Z|\n",
      "|retail_orders_in|0        |0     |ORD005  |P116      |NULL    |NULL |IN     |store  |2025-04-07T10:01:00Z|\n",
      "|retail_orders_in|0        |1     |ORD020  |P121      |NULL    |NULL |IN     |store  |2025-04-07T10:04:45Z|\n",
      "+----------------+---------+------+--------+----------+--------+-----+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "---- Batch 0 processed ----\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def debug_batch(df, epoch_id):\n",
    "    if df.count() == 0:\n",
    "        print(f\"---- Batch {epoch_id} is empty ----\")\n",
    "        return\n",
    "\n",
    "    print(f\"---- Processing Batch {epoch_id} ----\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .partitionBy(\"country\")\n",
    "        .save(\"delta/orders_by_country\")\n",
    "    )\n",
    "\n",
    "    print(f\"---- Batch {epoch_id} written to Delta ----\")\n",
    "\n",
    "# Start the streaming query\n",
    "query = (\n",
    "    df_parsed.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(debug_batch)\n",
    "    .option(\"checkpointLocation\", \"delta/checkpoints/orders_by_country_batch\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badcd2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for testing the parsed dataframe\n",
    "df_parsed.writeStream.format(\"console\").start().awaitTermination(10)\n",
    "query.stop()\n",
    "df_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d71e8",
   "metadata": {},
   "source": [
    "#### Query the Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e08f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|country|count(1)|\n",
      "+-------+--------+\n",
      "|     GE|      22|\n",
      "|     UK|      20|\n",
      "|     US|      18|\n",
      "|     IN|      16|\n",
      "|     FR|      13|\n",
      "|     CA|      11|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all = spark.read.format(\"delta\").load(\"delta/orders_by_country\")\n",
    "df_all.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "spark.sql(\"SELECT country, COUNT(*) FROM orders GROUP BY country\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6909f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+--------+----------+--------+-----+-------+-------+--------------------+\n",
      "|               topic|partition|offset|order_id|product_id|quantity|price|country|channel|           timestamp|\n",
      "+--------------------+---------+------+--------+----------+--------+-----+-------+-------+--------------------+\n",
      "|retail_orders_others|        1|     1|  ORD006|      P130|    null| null|     GE| online|2025-04-07T10:01:15Z|\n",
      "|retail_orders_others|        1|     3|  ORD010|      P109|    null| null|     GE| online|2025-04-07T10:02:15Z|\n",
      "|retail_orders_others|        1|     6|  ORD017|      P120|    null| null|     GE| online|2025-04-07T10:04:00Z|\n",
      "|retail_orders_others|        1|     7|  ORD018|      P137|    null| null|     GE| mobile|2025-04-07T10:04:15Z|\n",
      "|retail_orders_others|        1|     8|  ORD019|      P120|    null| null|     GE|  store|2025-04-07T10:04:30Z|\n",
      "|retail_orders_others|        1|     9|  ORD023|      P108|    null| null|     GE| online|2025-04-07T10:05:30Z|\n",
      "|retail_orders_others|        1|    10|  ORD030|      P109|    null| null|     GE| online|2025-04-07T10:07:15Z|\n",
      "|retail_orders_others|        1|    11|  ORD032|      P138|    null| null|     GE| mobile|2025-04-07T10:07:45Z|\n",
      "|retail_orders_others|        1|    16|  ORD045|      P114|    null| null|     GE| mobile|2025-04-07T10:11:00Z|\n",
      "|retail_orders_others|        1|    19|  ORD050|      P134|    null| null|     GE|  store|2025-04-07T10:12:15Z|\n",
      "|retail_orders_others|        1|    22|  ORD057|      P105|    null| null|     GE| mobile|2025-04-07T10:14:00Z|\n",
      "|retail_orders_others|        1|    24|  ORD064|      P105|    null| null|     GE| online|2025-04-07T10:15:45Z|\n",
      "|retail_orders_others|        1|    25|  ORD065|      P104|    null| null|     GE|  store|2025-04-07T10:16:00Z|\n",
      "|retail_orders_others|        1|    26|  ORD066|      P139|    null| null|     GE| mobile|2025-04-07T10:16:15Z|\n",
      "|retail_orders_others|        1|    29|  ORD072|      P109|    null| null|     GE|  store|2025-04-07T10:17:45Z|\n",
      "|retail_orders_others|        1|    30|  ORD073|      P146|    null| null|     GE|  store|2025-04-07T10:18:00Z|\n",
      "|retail_orders_others|        1|    32|  ORD076|      P147|    null| null|     GE| mobile|2025-04-07T10:18:45Z|\n",
      "|retail_orders_others|        1|    34|  ORD081|      P130|    null| null|     GE| mobile|2025-04-07T10:20:00Z|\n",
      "|retail_orders_others|        1|    35|  ORD082|      P100|    null| null|     GE|  store|2025-04-07T10:20:15Z|\n",
      "|retail_orders_others|        1|    37|  ORD090|      P118|    null| null|     GE| mobile|2025-04-07T10:22:15Z|\n",
      "+--------------------+---------+------+--------+----------+--------+-----+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3085a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_raw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCAST(value AS STRING)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\neelu\\myenv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:914\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    907\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    908\u001b[39m         message_parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m    911\u001b[39m         },\n\u001b[32m    912\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\neelu\\myenv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\neelu\\myenv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    171\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: Queries with streaming sources must be executed with writeStream.start();\nkafka"
     ]
    }
   ],
   "source": [
    "df_raw.selectExpr(\"CAST(value AS STRING)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc0c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
